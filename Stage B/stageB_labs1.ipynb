{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prompt Engineering Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install openai==v0.28.1\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_open_params(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,  \n",
    "):\n",
    "    \"\"\"set openai parameter\"\"\"\n",
    "    openai_params = {}    \n",
    "\n",
    "    openai_params['model'] = model\n",
    "    openai_params['temperature'] = temperature\n",
    "    openai_params['max_tokens'] = max_tokens\n",
    "    openai_params['top_p'] = top_p\n",
    "    openai_params['frequency_penalty'] = frequency_penalty\n",
    "    openai_params['presence_penalty'] = presence_penalty\n",
    "    return openai_params\n",
    "\n",
    "def get_completion(params, prompt):\n",
    "    \"\"\"Get completion from openai api\"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine = params['model'],\n",
    "        prompt = prompt,\n",
    "        temperature = params['temperature'],\n",
    "        max_tokens = params['max_tokens'],\n",
    "        top_p = params['top_p'],\n",
    "        frequency_penalty = params['frequency_penalty'],\n",
    "        presence_penalty = params['presence_penalty'],\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = set_open_params()\n",
    "\n",
    "prompt = \"Roses are\"\n",
    "\n",
    "response = get_completion(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' red, violets are blue\\n\\nSugar is sweet, and so are you.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " red, violets are blue\n",
       "\n",
       "Sugar is sweet, and so are you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with different temperature to compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " red, violets are blue\n",
       "Sugar is sweet, and so are you\n",
       "But the roses wilt, and the violets fade\n",
       "But my love for you will never degrade\n",
       "\n",
       "You are my sunshine on a cloudy day\n",
       "My guiding light when I lose my way\n",
       "With you by my side, I can conquer all\n",
       "Together we stand, we will never fall\n",
       "\n",
       "Your smile brightens up my darkest night\n",
       "Your touch fills me with pure delight\n",
       "I am grateful for every moment spent\n",
       "With you, my love, my heart is content\n",
       "\n",
       "I promise to love you, through thick and thin\n",
       "To be your rock, your shelter, your kin\n",
       "I'll hold your hand, and never let go\n",
       "Together we'll face whatever life may throw\n",
       "\n",
       "So here's my heart, my love, my all\n",
       "With you, I'll stand tall, I'll never fall\n",
       "Roses may wither, and violets may die\n",
       "But my love for you will never say goodbye."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = set_open_params(temperature=0)\n",
    "prompt = \"Roses are\"\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Keras is a user-friendly deep learning API written in Python that runs on top of TensorFlow, an open-source framework for performing mathematical computations and operations on numerical tensors using CPUs, GPUs, and TPUs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = set_open_params()\n",
    "prompt = \"\"\"Keras is a deep learning API written in Python that runs on top of TensorFlow. \n",
    "    It is quite popular among deep learning users because of its ease of use. \n",
    "    TensorFlow is an end-to-end open-source deep learning framework developed and maintained by Google. \n",
    "    Similar to Numpy, TensorFlow allows for mathematical computations and manipulation between numerical tensors, runs on CPUs, GPUs, and TPUs. \n",
    "    Keras was incorporated in TensorFlow 2.0 (the recent version) as tf.keras (high-level API) and can run on the aforementioned hardwares. \n",
    "    TensorFlow also allows for low-level operations with the TensorFlow Core API. \n",
    "\n",
    "    Explain the above in one sentence:\"\"\"\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To protect global stability from inner or extraterrestrial threats."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: The Avengers were a team of extraordinary individuals, with either superpowers or other special characteristics. Though primarily affiliated with the interests of the United States of America, the group's purpose was to protect global stability from inner or extraterrestrial threats. The Avengers were first assembled by S.H.I.E.L.D. as a result of the Avengers Initiative, when Loki invaded Earth with his Chitauri army. The team, consisting of Iron Man, Captain America, Hulk, Thor, Black Widow and Hawkeye defeated Loki and went their separate ways for a while.\n",
    "\n",
    "Question: Why were the Avengers formed?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
    "\n",
    "Text: I think the Avengers Endgame was an interesting movie..\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
    "\n",
    "Human: Hello, who are you?\n",
    "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the big bang theory?\n",
    "AI:\"\"\"\n",
    "\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "SELECT s.StudentId, s.StudentName FROM students s\n",
       "INNER JOIN departments d ON s.DepartmentId = d.DepartmentId\n",
       "WHERE d.DepartmentName = 'Computer Science'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a PostgreSQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
    "\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Then, explain why the result is even.\n",
       "\n",
       "Step 1: Identify the odd numbers\n",
       "Odd numbers are numbers that cannot be divided evenly by 2. In this group, the odd numbers are 15, 5, 13, 7, and 1.\n",
       "\n",
       "Step 2: Add the odd numbers\n",
       "15 + 5 + 13 + 7 + 1 = 41\n",
       "\n",
       "Step 3: Determine if the result is odd or even\n",
       "41 is an odd number.\n",
       "\n",
       "Step 4: Explain why the result is even\n",
       "Although the sum of the odd numbers is an odd number (41), the presence of the even number 32 in the group causes the overall sum to become even. This is because when an odd number is added to an even number, the result will always be an odd number. However, when two odd numbers are added together, the result will always be an even number. In this case, 41 (the sum of the odd numbers) plus 32 (an even number) equals 73, which is an odd number. However, when we add the remaining odd numbers (7 and 1) to 73, the result becomes 81, which is an odd number. Therefore, the presence of the even"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "response = get_completion(params, prompt)\n",
    "IPython.display.Markdown(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        #Insert system role and content here\n",
    "        #Insert user role and content here\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=30,\n",
    "    top_p=1\n",
    ") \n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud computing is a technology that allows users to access and store data, applications, and services over the internet instead of on a local hard drive or server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economy is growing steadily due to consumer spending and investments in renewable energy, leading to record low unemployment rates.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are to summarize the provided text into a concise form, maintaining the key points and context, briefly enough to fit as an MCQ option.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The economy has been growing steadily over the past few quarters, driven by increases in consumer spending and substantial investments in renewable energy infrastructure. Unemployment rates have also fallen to record lows as new industries are creating more jobs.\" \n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    top_p=1\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumer spending and investments in renewable energy have led to steady economic growth, resulting in low unemployment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ich liebe es zu reisen.\"\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do you say 'I love to travel' in German?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    "    top_p=1\n",
    ") \n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solar wind interacting with the Earth's magnetic field.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a knowledgeable assistant. A user will ask a question and you should provide a clear and concise answer briefly enough to fit as an MCQ option.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What causes the Northern Lights?\"\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=150,\n",
    "    top_p=1\n",
    ") \n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The internet was developed for research communication and resource sharing, now connecting millions globally. \n",
      "\n",
      "Option: Developed for research communication and resource sharing, now connecting millions\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Summarize the following text into a concise phrase suitable for an MCQ answer option.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The internet was developed to help researchers communicate and share computing resources. Today, it connects millions of people and devices across the globe.\"\n",
    "        }\n",
    "        #Insert user role and content here\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=30,\n",
    "        top_p=1\n",
    ") \n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify the following text's tone in a brief form that could fit as an MCQ option.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"The meeting was cancelled due to unforeseen circumstances.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=30,\n",
    "    top_p=1\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿A qué hora es la reunión?\n"
     ]
    }
   ],
   "source": [
    "#System content: \"Translate the following sentence into Spanish concisely to fit as an MCQ option.\"\n",
    "\n",
    "response =openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        #Insert system role and content here\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"What time is the meeting?\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Translate the following sentence into Spanish concisely to fit as an MCQ option.\"\n",
    "        }\n",
    "    ],\n",
    "temperature=0,\n",
    "max_tokens=30,\n",
    "top_p=1\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main cause of the American Civil War was disagreement over the institution of slavery.\n"
     ]
    }
   ],
   "source": [
    "#User content: \"I had an awful experience at the restaurant. The food was bland and the service was slow.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Provide a concise fact about the event in history mentioned by the user briefly enough to fit as an MCQ option.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What was the main cause of the American Civil War?\"\n",
    "        }\n",
    "],\n",
    "temperature=0,\n",
    "max_tokens =16,\n",
    "top_p=1,\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the text is negative.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify the sentiment of the text provided by the user as positive, negative, or neutral.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I had an awful experience at the restaurant. The food was bland and the service was slow.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=30,\n",
    "    top_p=1\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
